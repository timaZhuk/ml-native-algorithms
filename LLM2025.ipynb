{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1oG6wDvtHwJb9e77T48eMhqcMSUhXV15M",
      "authorship_tag": "ABX9TyPS6ZJYh6FM9HPEFbT6HQPH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timaZhuk/ml-native-algorithms/blob/main/LLM2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM from scratch (not Finished)"
      ],
      "metadata": {
        "id": "zlgeiACvkQIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKt8QmtTUBNh",
        "outputId": "72984d07-9eea-4600-8a56-cf161303d276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pylzma\n",
            "  Downloading pylzma-0.5.0.tar.gz (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pylzma\n",
            "  Building wheel for pylzma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylzma: filename=pylzma-0.5.0-cp310-cp310-linux_x86_64.whl size=222322 sha256=a86c8e361bb52dddd8eb7e4e874178504633b6b2ab425c64496c941310009f3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/c9/02/91112815e838f544c1d46fda071241e454694579d022751d2b\n",
            "Successfully built pylzma\n",
            "Installing collected packages: pylzma\n",
            "Successfully installed pylzma-0.5.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pylzma"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the file"
      ],
      "metadata": {
        "id": "YrOQVmKq-TR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ih9-iToWUBOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "block_size = 8\n",
        "batch_size = 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePhAgTQTb2cY",
        "outputId": "5a00efe9-760b-4347-860c-a6812a6f1ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.gutenberg.org/cache/epub/22566/pg22566.txt\n",
        "\n",
        "with open('/content/drive/MyDrive/wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "chars = sorted(set(text))\n",
        "print(chars)\n",
        "vocabulary_size = len(chars)\n",
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ck19RDvUETf",
        "outputId": "3195512f-9dd8-4156-b449-4f2b18581921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n",
            "252022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:150])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzGRfF2oUrS-",
        "outputId": "e4343cbb-4984-4611-84d1-0e55521a580a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of Dorothy and the Wizard in Oz\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other par\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number of individual charachters in the text"
      ],
      "metadata": {
        "id": "KMddyaaG-bfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(set(text))\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTgqdzxmUrVl",
        "outputId": "fe768fe0-ca77-4dd8-9628-4a161f5f72cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode char to integer\n",
        "string_to_int = {ch:i for i, ch in enumerate(chars)}\n",
        "# decode int to char\n",
        "int_to_string  = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])"
      ],
      "metadata": {
        "id": "wUesYFFgUEWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode 'hello' to array of integers\n",
        "print(encode('hello'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrqioSNqUEZv",
        "outputId": "fea60b48-47e9-40be-f78e-0d8449e222dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[65, 62, 69, 69, 72]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decode [65, 62, 69, 69, 72] array to string\n",
        "print(decode([65, 62, 69, 69, 72]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9aKlFteUEce",
        "outputId": "9bee29e0-60ed-4c9d-ce35-3c86177ed142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to pytorch tensor\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvgfeWu2RXLw",
        "outputId": "f5c4b357-c392-4b3a-c57e-8114be023a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([91, 48, 65, 62,  1, 44, 75, 72, 67, 62, 60, 77,  1, 35, 78, 77, 62, 71,\n",
            "        59, 62, 75, 64,  1, 62, 30, 72, 72, 68,  1, 72, 63,  1, 32, 72, 75, 72,\n",
            "        77, 65, 82,  1, 58, 71, 61,  1, 77, 65, 62,  1, 51, 66, 83, 58, 75, 61,\n",
            "         1, 66, 71,  1, 43, 83,  0,  1,  1,  1,  1,  0, 48, 65, 66, 76,  1, 62,\n",
            "        59, 72, 72, 68,  1, 66, 76,  1, 63, 72, 75,  1, 77, 65, 62,  1, 78, 76,\n",
            "        62,  1, 72, 63,  1, 58, 71, 82, 72, 71])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into train and validation\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "6-JrC6RSRXOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram models shifting training data\n",
        "block_size = 8\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print('when input is', context, 'target is', target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEmotcBURXVI",
        "outputId": "d4f10662-6935-4f37-95f9-869a291ab499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([91]) target is tensor(48)\n",
            "when input is tensor([91, 48]) target is tensor(65)\n",
            "when input is tensor([91, 48, 65]) target is tensor(62)\n",
            "when input is tensor([91, 48, 65, 62]) target is tensor(1)\n",
            "when input is tensor([91, 48, 65, 62,  1]) target is tensor(44)\n",
            "when input is tensor([91, 48, 65, 62,  1, 44]) target is tensor(75)\n",
            "when input is tensor([91, 48, 65, 62,  1, 44, 75]) target is tensor(72)\n",
            "when input is tensor([91, 48, 65, 62,  1, 44, 75, 72]) target is tensor(67)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oazfIk-1ZDwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQO6GqDMZDzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IR8MRm6dPwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to PyTorch"
      ],
      "metadata": {
        "id": "OietVYe8dQFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "mIQODSMhdUsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randint = torch.randint(-100, 100, (6,))\n",
        "randint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vueKZodJdUvH",
        "outputId": "ae1963d3-b683-4194-c97d-0f014c80303b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 93, -86,  16,  19, -15,  66])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([\n",
        "    [0.1, 1.2],\n",
        "    [2.2, 3.1],\n",
        "    [4.9, 5.2]\n",
        "])\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wZIs3nadUyJ",
        "outputId": "cd7e9385-756a-4bcc-f976-b44ca485b9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1000, 1.2000],\n",
              "        [2.2000, 3.1000],\n",
              "        [4.9000, 5.2000]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create tensor of zeros\n",
        "zeros = torch.zeros(2, 3)\n",
        "zeros"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4n7e-kJd0qA",
        "outputId": "8052505a-796c-4e19-8655-05ab138656b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ones = torch.ones(3, 4)\n",
        "ones"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcxr2R-od0sX",
        "outputId": "fbbb4144-a22b-4a32-8b15-08cb60b5eb17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.empty(2, 3)\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCVHSFahegwd",
        "outputId": "6f6d9ec9-f5be-48c3-ce78-2c549778637f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.1707e-18, 7.0952e+22, 1.7748e+28],\n",
              "        [1.8176e+31, 7.2708e+31, 5.0778e+31]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arange = torch.arange(5)\n",
        "arange"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lur_bIZ_eg0W",
        "outputId": "e95b7bcc-dd99-478e-c82c-9e6e7cc39f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linespace = torch.linspace(3, 10, steps = 5)\n",
        "linespace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z16waMAmeg39",
        "outputId": "8c54f96b-100f-44b1-b654-1215cf1e3ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logspace = torch.logspace(start = -10, end = 10, steps = 5)"
      ],
      "metadata": {
        "id": "Fe--X7BSfViE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eye = torch.eye(5)\n",
        "eye"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xdJ0pJdfVk7",
        "outputId": "6b9856c9-a0f0-4ab9-cf53-0f9d0052b8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eye.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1yBEWaDfVnz",
        "outputId": "cb38362f-d5a6-49ca-9f5b-097e9165bac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.empty((2, 3), dtype=torch.int64)\n",
        "empty_like = torch.empty_like(a)\n",
        "empty_like"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53dVKIK018f5",
        "outputId": "484df748-a76c-4727-edc9-b5814b881c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[94471603080000, 94471604008000,    -4294967295],\n",
              "        [    8589934592,              3,            113]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next section. Time perfomance"
      ],
      "metadata": {
        "id": "NWDftZrr3QzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dzKRluL18io",
        "outputId": "d30c9a5b-f0d8-4617-9f4b-4259d165519e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# matrix operations here\n",
        "zeros = torch.zeros(1, 1)\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"{elapsed_time:.4f} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYxNWsF83TYN",
        "outputId": "165badb1-6a27-4536-fae5-32954a8a689a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0258 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the numpy and pyTorch"
      ],
      "metadata": {
        "id": "z2YrIf075PJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
        "torch_rand2 = torch.rand(10000, 10000).to(device)\n",
        "#\n",
        "np_rand1 = torch.rand(10000, 10000)\n",
        "np_rand2 = torch.rand(10000, 10000)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rand = (torch_rand1 @ torch_rand2)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"{elapsed_time:.4f} \")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rand = np.multiply(np_rand1, np_rand2)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"{elapsed_time:.4f} \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXhc2qvi3TbO",
        "outputId": "3c641d66-0d2b-4f40-e973-d2dec9019472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31.6590 \n",
            "0.2011 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.stack, torch.multinominal, torch.tril\n",
        "# torch.triu, input.T/input.transpose, nn.linear\n",
        "# torch.cat, F.softmax (show all the examples of functions/)\n",
        "\n",
        "# Define aprobability tensor\n",
        "probabilities = torch.tensor([0.1, 0.9])\n",
        "# 10% or 0.1 => 0, 90% or 0.9 => 1 each probability points to the index of the probability in the tensor\n",
        "# Draw 5 samples from the multinominal distribution\n",
        "samples = torch.multinomial(probabilities, num_samples = 10, replacement = True)\n",
        "print(samples )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGE2t5U_4uxl",
        "outputId": "cab8258b-1900-4a8e-fba8-d09bd2230e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the tensor together\n",
        "tensor = torch.tensor([1, 2, 3, 4])\n",
        "out = torch.cat((tensor, torch.tensor([5])), dim=0)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovywhjZE4u0E",
        "outputId": "ec120c90-5c97-4ddf-a529-9f105d6aa41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lower trangle\n",
        "out = torch.tril(torch.ones(5, 5))\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuGk_E8618lS",
        "outputId": "e661c930-9999-4dbf-8b03-f762a5dcec34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upper trangle matrix\n",
        "out = torch.triu(torch.ones(5, 5))\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7C85mMNdU0z",
        "outputId": "f273b1b5-8a07-4779-babb-a0000d34daae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [0., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.zeros(5, 5).masked_fill(torch.tril(torch.ones(5,5))==0, float('-inf'))\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkEOliadCgrT",
        "outputId": "521a1755-1bd5-4f12-fe18-0db96294c16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(out)"
      ],
      "metadata": {
        "id": "JZXmCcvRCgxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ae85fe-477f-418c-ab73-4795152d1170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.zeros(2, 3, 4)\n",
        "input"
      ],
      "metadata": {
        "id": "WnSrZ2RvCg1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b139c2e-d951-454e-96d0-a8bccd6ef18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out= input.transpose(0, 2)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFGJMd3sJevR",
        "outputId": "067fbafa-4d59-412a-c2c7-cd1531cb62b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7Wu7r2NJe0I",
        "outputId": "a75f8d07-48af-4f31-d953-c1607b4c7f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.tensor([1, 2, 3])\n",
        "tensor2 = torch.tensor([4, 5, 6])\n",
        "tensor3 = torch.tensor([7, 8, 9])"
      ],
      "metadata": {
        "id": "bhkWkSQ3Je45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack the tensors along a new dimension\n",
        "stacked_tensor =  torch.stack([tensor1, tensor2, tensor3])\n",
        "stacked_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UINtrF8jKRc_",
        "outputId": "38380fbb-2665-453c-8c12-8fd1f7846070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6],\n",
              "        [7, 8, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "sample = torch.tensor([10.,10.,10.])\n",
        "linear = nn.Linear(3, 3, bias=False)\n",
        "print(linear(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNKBxJCzKRgS",
        "outputId": "f4e05bb6-f181-4054-a719-7d93bd19f592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.0519,  7.5186, -6.0741], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "#Create a tensor\n",
        "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "# Apply softmax using torch.nn.functional.softmax()\n",
        "softmax_output = F.softmax(tensor1, dim=0)\n",
        "print(softmax_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd9S63_bLyR7",
        "outputId": "ab89dd05-dab1-43ba-ceb0-e49ca699c25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 100000\n",
        "embedding_dim = 100\n",
        "embeding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "#Create some input indices\n",
        "input_indices = torch.LongTensor([1, 5, 3, 2])\n",
        "\n",
        "#Apply the embedding layer\n",
        "embedded_output = embeding(input_indices)\n",
        "\n",
        "# the output will be a tensor of shape (4, 100) where 4 is the number of inputs\n",
        "# end 100 is the dimensionality of the embeding vectors\n",
        "print(embedded_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G08LR5r4LyUc",
        "outputId": "848ed68c-6235-4ebc-eac2-ba4ce5bbf5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([\n",
        "    [1,2],\n",
        "    [3,4],\n",
        "    [5,6]\n",
        "    ])\n",
        "\n",
        "b = torch.tensor([\n",
        "    [7,8,9],\n",
        "    [10,11,12]\n",
        "])\n",
        "\n",
        "print(a @ b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8L85iuxS0Vc",
        "outputId": "9d7d5e84-2c29-498e-f10a-fcdc5bc87c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 27,  30,  33],\n",
            "        [ 61,  68,  75],\n",
            "        [ 95, 106, 117]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# type int_64\n",
        "int_64 = torch.randint(1, (3,2)).float()\n",
        "\n",
        "# type float 32\n",
        "float_32 = torch.rand(2,3)\n",
        "\n",
        "result = torch.matmul(int_64, float_32)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUqjF0xBS0bF",
        "outputId": "7130fcca-34e2-429c-c4f7-3f479d66ecab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batcj sizing recap model"
      ],
      "metadata": {
        "id": "x_1wLV43p7Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "print(device)\n",
        "block_size = 8\n",
        "batch_size = 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwN5HTgkS0fD",
        "outputId": "f8f8bacb-79f2-4906-8f13-256a954879ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.gutenberg.org/cache/epub/22566/pg22566.txt\n",
        "\n",
        "with open('/content/drive/MyDrive/wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "chars = sorted(set(text)) # only unique characters in some order\n",
        "print(chars)\n",
        "vocabulary_size = len(chars)\n",
        "print(len(text))\n",
        "print(vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrZKjS_-pdOG",
        "outputId": "f1e5bd3a-c972-46a2-8c64-8ce33a27b86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n",
            "252022\n",
            "92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary where to each 'char' corresponed to integer(index)\n",
        "string_to_int = {ch:i for i, ch in enumerate(chars)}\n",
        "\n",
        "# Dictionary where to each 'integer' we get 'char'\n",
        "int_to_string = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# encode string to numerical array\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "\n",
        "# decode array of numbers to the string\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
        "\n",
        "#example\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT6mWpNwpdSw",
        "outputId": "41797a95-7fe5-4802-a25f-9241f1c3c496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([91, 48, 65, 62,  1, 44, 75, 72, 67, 62, 60, 77,  1, 35, 78, 77, 62, 71,\n",
            "        59, 62, 75, 64,  1, 62, 30, 72, 72, 68,  1, 72, 63,  1, 32, 72, 75, 72,\n",
            "        77, 65, 82,  1, 58, 71, 61,  1, 77, 65, 62,  1, 51, 66, 83, 58, 75, 61,\n",
            "         1, 66, 71,  1, 43, 83,  0,  1,  1,  1,  1,  0, 48, 65, 66, 76,  1, 62,\n",
            "        59, 72, 72, 68,  1, 66, 76,  1, 63, 72, 75,  1, 77, 65, 62,  1, 78, 76,\n",
            "        62,  1, 72, 63,  1, 58, 71, 82, 72, 71])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data on tarining and validations sets\n",
        "n = int(0.8*len(data))\n",
        "#\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "oE41pvT7pdX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) # make a tensor of randomly chosen [indeces from 'data']\n",
        "  print(ix)\n",
        "  # stack horisontally for x\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  # for y shifted by 1\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print('inputs, x:')\n",
        "print(x.shape)\n",
        "print(x)\n",
        "print('trgets, y:')\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyzmUw_QrUPC",
        "outputId": "e9a7778c-f84d-4909-8822-ebf1faabfa02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([163794, 151814, 185723, 120998])\n",
            "inputs, x:\n",
            "torch.Size([4, 8])\n",
            "tensor([[61,  1, 54, 62, 59, 12,  1, 58],\n",
            "        [66, 71, 64,  1, 58,  1, 80, 66],\n",
            "        [ 0,  0,  3, 36, 72, 80,  1, 80],\n",
            "        [68, 82,  1, 76, 66, 61, 62, 76]])\n",
            "trgets, y:\n",
            "tensor([[ 1, 54, 62, 59, 12,  1, 58, 76],\n",
            "        [71, 64,  1, 58,  1, 80, 66, 61],\n",
            "        [ 0,  3, 36, 72, 80,  1, 80, 62],\n",
            "        [82,  1, 76, 66, 61, 62, 76,  1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is: {context} target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX66ziCurUUC",
        "outputId": "b5c5f005-bfce-4bff-e742-92e8adc58eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is: tensor([91]) target is 48\n",
            "when input is: tensor([91, 48]) target is 65\n",
            "when input is: tensor([91, 48, 65]) target is 62\n",
            "when input is: tensor([91, 48, 65, 62]) target is 1\n",
            "when input is: tensor([91, 48, 65, 62,  1]) target is 44\n",
            "when input is: tensor([91, 48, 65, 62,  1, 44]) target is 75\n",
            "when input is: tensor([91, 48, 65, 62,  1, 44, 75]) target is 72\n",
            "when input is: tensor([91, 48, 65, 62,  1, 44, 75, 72]) target is 67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information:\n",
        "### A bigram language model predicts the next word (or character) in a sequence based on the previous word (or character). It considers pairs of consecutive items (bigrams). For example, in the sentence \"the cat sat,\" the bigrams are \"the cat,\" \"cat sat.\"\n",
        "# Embedding Vectors (in this specific model):\n",
        "\n",
        "### In this particular bigram model implementation, the embedding vectors have the same dimension as the vocabulary size. This means that the embedding vector for each token is a one-hot vector (or close to it after training) representing the probability distribution over the next word. For example, if after training the embedding vector for \"cat\" is [0.1, 0.6, 0.15, 0.05, 0.1], it means that the model predicts the next word to be \"cat\" with 60% probability, \"the\" with 10% probability, \"sat\" with 15% probability, and so on.\n",
        "## * token_embedding_table\n",
        "### The self.token_embedding_table is an nn.Embedding layer. You can think of it as a lookup table. It maps each token (represented by its numerical index) to a corresponding vector (the embedding vector).\n",
        "\n",
        "### xample: Let's say your vocabulary has 5 words: \"the,\" \"cat,\" \"sat,\" \"on,\" \"mat.\" You would assign each word a unique index (e.g., \"the\": 0, \"cat\": 1, \"sat\": 2, \"on\": 3, \"mat\": 4). The token_embedding_table would be a 5x5 matrix (in this case, because the embedding dimension is equal to the vocabulary size). Each row of this matrix is the embedding vector for the corresponding word.\n",
        "## * Logits:\n",
        "### In the context of classification (which language modeling essentially is), logits are the raw, unnormalized outputs of the model before applying a softmax function. In this bigram model, the logits directly represent the model's prediction of the next token.\n",
        "\n",
        "### Example: If the input index is 1 (representing the word \"cat\"), self.token_embedding_table(1) will retrieve the embedding vector for \"cat\" (which is a vector of length 5 in this model). This vector is the logit and can be interpreted as the model's \"preference\" for each word in the vocabulary to be the next word.\n",
        "\n",
        "# Why Use Embeddings?\n",
        "\n",
        "### Semantic Similarity: Similar words have similar embedding vectors. This allows models to understand relationships between words.\n",
        "### Dimensionality Reduction: Embeddings are much smaller than one-hot vectors, making computations more efficient.\n",
        "Generalization: Embeddings allow models to generalize better to unseen words or contexts.\n",
        "# Character-Level Embeddings:\n",
        "\n",
        "### Character-level embeddings represent individual characters as vectors. This is particularly useful for:\n",
        "\n",
        "### Handling Out-of-Vocabulary (OOV) words: If a word is not in the model's vocabulary, it can still be represented as a sequence of character embeddings.\n",
        "### Morphological information: Character embeddings can capture prefixes, suffixes, and other morphological features of words.\n",
        "### Languages with rich morphology: Languages with many word inflections benefit from character-level representations."
      ],
      "metadata": {
        "id": "DH5Fs5vMLCeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward() method\n",
        "### index: This is the input tensor containing the indices of the input tokens. It has a shape of (B, T), where:\n",
        "\n",
        "## B is the batch size (the number of sequences processed in parallel).\n",
        "## T is the sequence length (the number of tokens in each sequence).\n",
        "## logits = ...: This line performs the embedding lookup. It takes the input indices and retrieves the corresponding embedding vectors from the token_embedding_table. The resulting logits tensor has a shape of (B, T, C), where C is the embedding dimension (or vocabulary size in a simplified bigram model).\n",
        "\n",
        "## logits.view(B*T, C) reshapes the logits tensor. This is necessary to prepare the logits for the F.cross_entropy loss function.\n",
        "# It combines the batch and time dimensions into a single dimension. The new shape is (B*T, C).\n",
        "\n",
        "# Example (continuing from the previous example): logits was (2, 2, 3). After view(), it becomes (4, 3)\n",
        "\n",
        "# targets = targets.view(B*T):\n",
        "\n",
        "## targets: This tensor contains the indices of the target tokens (the next tokens in the sequence that the model should predict). It has a shape of (B, T).\n",
        "\n",
        "## targets.view(B*T) reshapes the targets tensor to have a shape of (B*T). This is also necessary for the F.cross_entropy loss function. It flattens the target tensor into a 1D tensor.\n",
        "\n",
        "## Example: If targets was [[2, 4], [0, 1]] (meaning the next tokens should be at indices 2, 4 in the first sequence and 0, 1 in the second sequence), after view(), it becomes [2, 4, 0, 1].\n",
        "# 5. loss = F.cross_entropy(logits, targets):\n",
        "\n",
        "# F.cross_entropy(logits, targets) calculates the cross-entropy loss between the predicted logits and the target indices.\n",
        "\n",
        "# The cross_entropy loss function is commonly used for multi-class classification problems (like language modeling, where each word in the vocabulary is a class)."
      ],
      "metadata": {
        "id": "fv__FvS-XPqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguagemodel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # 5x5 matrix for example\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "    logits = self.token_embedding_table(index)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      print(B, T, C)\n",
        "      logits = logits.view(B*T, C)\n",
        "      print(\"logits shape: \",logits.shape)\n",
        "      targets = targets.view(B*T)\n",
        "      print(\"targets shape: \",targets.shape)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, index, max_new_tokens):\n",
        "    # index is (B, T) array of indecies in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions\n",
        "      logits, loss = self.forward(index)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1 , :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim = -1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sample index to the running sequence\n",
        "      index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
        "    return index"
      ],
      "metadata": {
        "id": "8Cv8wxCACg-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguagemodel(vocabulary_size)\n",
        "m = model.to(device)\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device= device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWx_ZEzo-jl4",
        "outputId": "a7dcfc18-5336-4b92-8ac0-5e444612c782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "$v5-\n",
            "(XHT?‘O XorGNxQib(E—w]Qr,#”u ;’%d3C‘btKno sRC﻿f;\"lXNu 30!C—Mrg“AoY;2Bwz3Y?iVHnPfOS9B[5mSjM:PksGABp*4m90!™&%™PwCP8”TIS‘.BpjZ.—f™mjn6”﻿HT0uH—uHu‘5%81YaS96n.z’RPl\n",
            "﻿\n",
            ";PZf1;XIY.J,jdqe2z™FO*﻿—I.-XHZ;jZ1R)z—tsF5LHh''xJ2pHiYm6OuE?'?dfO[3—Eav’—“2-”.[!p!jT 8m.-]8TY0m\"T™Ma56-—vpMN_JasT7Ph—#;QJ,d;xE#‘XaV59hSQ;UCQXegxrLN;NxRBSm0$6-'-&ZP,\",1Q-L8%\"M4BR1hg ‘k•wIoS#Q•tPu,W$[‘\n",
            "P•FCUhLGf$6%eLm™”Z7fjd﻿)jci)jR”$x6WaK.Wv0?F1“vZa:OEC,vFy0vTkVPGTF1gqO')U]/\n",
            "•7Yf]•trjF‘l—ZfFUa5WmMG5]0d﻿[8%LS﻿41.(:f!p﻿Lp_%TL[$.—rj-';\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a.view(x, y, z) is the key part. The view() function reshapes a tensor without copying the underlying data. It creates a new \"view\" of the same data in memory.\n",
        "## In this specific case, a.view(x, y, z) is called with the same dimensions as the original tensor. This essentially does nothing. The shape remains (2, 3, 5).\n",
        "# Important Considerations:\n",
        "\n",
        "## view() creates a view, not a copy. Changes to the view will affect the original tensor, and vice versa.\n",
        "## The total number of elements must remain the same when using view(). You can't change the number of elements.\n",
        "## Sometimes, if the tensor is not contiguous in memory (due to previous operations like transpose() or permute()), you might need to call contiguous() before view() to ensure it works correctly.\n",
        "##"
      ],
      "metadata": {
        "id": "rsK0uuGaU5Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(2, 3, 5)\n",
        "print(a.shape)\n",
        "x, y, z = a.shape\n",
        "print(x, y, z)\n",
        "a = a.view(x, y, z)\n",
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6S3gXrf0I0-",
        "outputId": "fde49826-27aa-4a56-ba1c-12eb15bb2488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 5])\n",
            "2 3 5\n",
            "torch.Size([2, 3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmMqBVab0I7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWZZfWUw0JAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}